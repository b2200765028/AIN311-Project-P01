{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b2200765028/AIN311-Project-P01/blob/main/ain311_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8d8aa5",
      "metadata": {
        "id": "6f8d8aa5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from datasets import list_datasets, load_dataset\n",
        "from pprint import pprint\n",
        "import random\n",
        "import keras\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e89eaa",
      "metadata": {
        "id": "64e89eaa",
        "outputId": "8383a2cc-32db-4f24-9119-6f6d60ec74bf",
        "colab": {
          "referenced_widgets": [
            "bdbfcabd8cb04853bcc06ce293184946"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset squad (C:/Users/Huawei/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdbfcabd8cb04853bcc06ce293184946",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset('squad') ## downloading squad dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7d26853",
      "metadata": {
        "id": "e7d26853",
        "outputId": "b421599e-ce39-418f-9bcc-412718afc1e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached shuffled indices for dataset at C:/Users/Huawei/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-10de9997c4b83f65.arrow\n",
            "Loading cached shuffled indices for dataset at C:/Users/Huawei/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-f3a033b6ac26514f.arrow\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 87599\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10570\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 327,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset.shuffle(seed=42)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a653459",
      "metadata": {
        "id": "6a653459"
      },
      "outputs": [],
      "source": [
        "train_squad = pd.DataFrame(dataset[\"train\"][:2000])\n",
        "test_squad = pd.DataFrame(dataset[\"validation\"][:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8e6846",
      "metadata": {
        "id": "9d8e6846",
        "outputId": "a7d027f7-7707-46af-87e1-bdb4b817b1dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2000 entries, 0 to 1999\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        2000 non-null   object\n",
            " 1   title     2000 non-null   object\n",
            " 2   context   2000 non-null   object\n",
            " 3   question  2000 non-null   object\n",
            " 4   answers   2000 non-null   object\n",
            "dtypes: object(5)\n",
            "memory usage: 78.2+ KB\n"
          ]
        }
      ],
      "source": [
        "train_squad.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca8efa9",
      "metadata": {
        "id": "aca8efa9",
        "outputId": "182062c6-5627-4802-cbd4-04501c0ac2de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>573173d8497a881900248f0c</td>\n",
              "      <td>Egypt</td>\n",
              "      <td>The Pew Forum on Religion &amp; Public Life ranks ...</td>\n",
              "      <td>What percentage of Egyptians polled support de...</td>\n",
              "      <td>{'text': ['84%'], 'answer_start': [468]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57277e815951b619008f8b52</td>\n",
              "      <td>Ann_Arbor,_Michigan</td>\n",
              "      <td>The Ann Arbor Hands-On Museum is located in a ...</td>\n",
              "      <td>Ann Arbor ranks 1st among what goods sold?</td>\n",
              "      <td>{'text': ['books'], 'answer_start': [402]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5727e2483acd2414000deef0</td>\n",
              "      <td>Rule_of_law</td>\n",
              "      <td>One important aspect of the rule-of-law initia...</td>\n",
              "      <td>In developing countries, who makes most of the...</td>\n",
              "      <td>{'text': ['the executive'], 'answer_start': [6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5728f5716aef0514001548cc</td>\n",
              "      <td>Samurai</td>\n",
              "      <td>In December 1547, Francis was in Malacca (Mala...</td>\n",
              "      <td>Who impressed Xavier by taking notes in church?</td>\n",
              "      <td>{'text': ['Anjiro'], 'answer_start': [160]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>572826002ca10214002d9f16</td>\n",
              "      <td>Group_(mathematics)</td>\n",
              "      <td>Groups are also applied in many other mathemat...</td>\n",
              "      <td>What represents elements of the fundamental gr...</td>\n",
              "      <td>{'text': ['loops'], 'answer_start': [489]}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         id                title  \\\n",
              "0  573173d8497a881900248f0c                Egypt   \n",
              "1  57277e815951b619008f8b52  Ann_Arbor,_Michigan   \n",
              "2  5727e2483acd2414000deef0          Rule_of_law   \n",
              "3  5728f5716aef0514001548cc              Samurai   \n",
              "4  572826002ca10214002d9f16  Group_(mathematics)   \n",
              "\n",
              "                                             context  \\\n",
              "0  The Pew Forum on Religion & Public Life ranks ...   \n",
              "1  The Ann Arbor Hands-On Museum is located in a ...   \n",
              "2  One important aspect of the rule-of-law initia...   \n",
              "3  In December 1547, Francis was in Malacca (Mala...   \n",
              "4  Groups are also applied in many other mathemat...   \n",
              "\n",
              "                                            question  \\\n",
              "0  What percentage of Egyptians polled support de...   \n",
              "1         Ann Arbor ranks 1st among what goods sold?   \n",
              "2  In developing countries, who makes most of the...   \n",
              "3    Who impressed Xavier by taking notes in church?   \n",
              "4  What represents elements of the fundamental gr...   \n",
              "\n",
              "                                             answers  \n",
              "0           {'text': ['84%'], 'answer_start': [468]}  \n",
              "1         {'text': ['books'], 'answer_start': [402]}  \n",
              "2  {'text': ['the executive'], 'answer_start': [6...  \n",
              "3        {'text': ['Anjiro'], 'answer_start': [160]}  \n",
              "4         {'text': ['loops'], 'answer_start': [489]}  "
            ]
          },
          "execution_count": 330,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_squad.head()\n",
        "## we dont need id gonna drop it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3beea66c",
      "metadata": {
        "id": "3beea66c",
        "outputId": "76521d3b-df15-40e0-f31e-78aa01bd6cda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1970s']"
            ]
          },
          "execution_count": 331,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_squad[\"answers\"][12][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e25c00",
      "metadata": {
        "id": "e3e25c00"
      },
      "outputs": [],
      "source": [
        "train_squad.drop(\"id\",axis=1,inplace=True)\n",
        "test_squad.drop(\"id\",axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d27da01",
      "metadata": {
        "id": "9d27da01",
        "outputId": "851ce955-8f8f-4550-a7c6-8ff267eeccf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2008_Sichuan_earthquake                         24\n",
              "New_York_City                                   18\n",
              "Beyoncé                                         17\n",
              "Queen_Victoria                                  17\n",
              "Roman_Republic                                  16\n",
              "                                                ..\n",
              "National_Archives_and_Records_Administration     1\n",
              "Pesticide                                        1\n",
              "General_Electric                                 1\n",
              "Turner_Classic_Movies                            1\n",
              "Glacier                                          1\n",
              "Name: title, Length: 416, dtype: int64"
            ]
          },
          "execution_count": 333,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_squad[\"title\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41688f63",
      "metadata": {
        "id": "41688f63",
        "outputId": "d5ab1dd1-9ccb-45d3-a11b-48717ece0dc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(120, 10)"
            ]
          },
          "execution_count": 334,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "avg_words_context = round(sum([len(i.split()) for i in train_squad[\"context\"]])/len(train_squad))\n",
        "avg_words_question = round(sum([len(i.split()) for i in train_squad[\"question\"]])/len(train_squad))\n",
        "avg_words_context,avg_words_question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc56063",
      "metadata": {
        "id": "ddc56063",
        "outputId": "5aca5223-b793-47c6-b85d-cfb51263a616"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(486, 26)"
            ]
          },
          "execution_count": 335,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_words_context = (max([len(i.split()) for i in train_squad[\"context\"]]))\n",
        "max_words_question = (max([len(i.split()) for i in train_squad[\"question\"]]))\n",
        "max_words_context,max_words_question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aea34046",
      "metadata": {
        "id": "aea34046"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b10f5434",
      "metadata": {
        "id": "b10f5434"
      },
      "outputs": [],
      "source": [
        "encoder_text = train_squad[\"context\"]\n",
        "decoder_text = train_squad[\"question\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce802b5d",
      "metadata": {
        "id": "ce802b5d"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "text = encoder_text + decoder_text\n",
        "tokenizer.fit_on_texts(text)\n",
        "dictionary = tokenizer.word_index\n",
        "    \n",
        "word2i ={k:v for (k,v) in dictionary.items()}\n",
        "i2word = {v:k for (k,v) in word2i.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea6232d",
      "metadata": {
        "id": "9ea6232d",
        "outputId": "d7e0db83-6b53-4a00-aaa4-93edfd42e739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most occured 10 words in vocab\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['the', 'of', 'and', 'in', 'to', 'a', 'is', 'was', 'as', 'for']"
            ]
          },
          "execution_count": 338,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Most occured 10 words in vocab\")\n",
        "sorted(tokenizer.word_counts,key=tokenizer.word_counts.get,reverse=True)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893bd550",
      "metadata": {
        "id": "893bd550",
        "outputId": "097ec113-b0f4-478c-ac87-fc997aeac7e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26185"
            ]
          },
          "execution_count": 339,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size= len(word2i)+1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbb489b5",
      "metadata": {
        "id": "bbb489b5"
      },
      "outputs": [],
      "source": [
        "encoder_seq = tokenizer.texts_to_sequences(encoder_text)\n",
        "decoder_seq = tokenizer.texts_to_sequences(decoder_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "299b0858",
      "metadata": {
        "id": "299b0858",
        "outputId": "238398d9-431d-4f7d-ab7e-da62106f2e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text \n",
            "-----------\n",
            " In July 1855, the allied squadron tried to go past Taganrog to Rostov on Don, entering the Don River through the Mius River. On 12 July 1855 HMS Jasper grounded near Taganrog thanks to a fisherman who moved buoys into shallow water. The Cossacks captured the gunboat with all of its guns and blew it up. The third siege attempt was made 19–31 August 1855, but the city was already fortified and the squadron could not approach close enough for landing operations. The allied fleet left the Gulf of Taganrog on the 2nd September 1855, with minor military operations along the Azov Sea coast continuing until late autumn 1855.\n",
            "Turned into sequences \n",
            "-----------\n",
            " [4, 340, 3458, 1, 1378, 6289, 1665, 5, 811, 1074, 7584, 5, 9734, 14, 5398, 5399, 1, 5398, 308, 88, 1, 14021, 308, 14, 434, 340, 3458, 9735, 9736, 4715, 456, 7584, 4220, 5, 6, 14022, 34, 673, 14023, 51, 4221, 270, 1, 14024, 1666, 1, 14025, 13, 52, 2, 32, 2382, 3, 6290, 21, 114, 1, 290, 3459, 934, 8, 99, 14026, 435, 3458, 39, 1, 49, 8, 1152, 7585, 3, 1, 6289, 146, 30, 1203, 810, 1204, 10, 2105, 843, 1, 1378, 3170, 482, 1, 2383, 2, 7584, 14, 1, 2939, 436, 3458, 13, 1756, 199, 843, 207, 1, 14027, 600, 901, 2553, 130, 202, 3797, 3458]\n"
          ]
        }
      ],
      "source": [
        "print(\"Original Text \\n-----------\\n\",encoder_text[10])\n",
        "print(\"Turned into sequences \\n-----------\\n\",encoder_seq[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e623f3da",
      "metadata": {
        "id": "e623f3da"
      },
      "source": [
        "### we are going to find max length of sequences and I will use padding to make them equal size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeb4df62",
      "metadata": {
        "id": "eeb4df62"
      },
      "outputs": [],
      "source": [
        "max_len_enc = 0\n",
        "max_len_dec = 0\n",
        "\n",
        "for enc,dec in zip(encoder_seq,decoder_seq):\n",
        "    if len(enc) > max_len_enc:\n",
        "        max_len_enc = len(enc)\n",
        "    if len(dec) > max_len_dec:\n",
        "        max_len_dec= len(dec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef37fa3",
      "metadata": {
        "id": "8ef37fa3",
        "outputId": "3f12d21b-cd9e-469a-e28a-ff0e2dcb7ffe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(498, 26)"
            ]
          },
          "execution_count": 343,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len_enc,max_len_dec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a96a55",
      "metadata": {
        "id": "70a96a55"
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences\n",
        "encoder_data = pad_sequences(encoder_seq,maxlen=max_len_enc,dtype=\"int32\",padding=\"post\",truncating=\"post\")\n",
        "decoder_data = pad_sequences(decoder_seq,maxlen=max_len_dec,dtype=\"int32\",padding=\"post\",truncating=\"post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452db2db",
      "metadata": {
        "id": "452db2db"
      },
      "source": [
        "#### as we can see it added extra zeros to make them same size(we selected post to add zeros after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c75656",
      "metadata": {
        "id": "59c75656",
        "outputId": "cb6c2a35-1f2d-4ac6-ad4a-d3bceb21f96d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   32,  3160,   929,     2,   182,    12,  1047,   810,  3788,\n",
              "         196,     5,     1,   687,     3,  1238,     2,  6264,     2,\n",
              "         277,  1370,  4202,     4,    22, 13960,    11,     1,    84,\n",
              "         181,     5,     6,   301,  7560,   518,  4695,  1969,  6265,\n",
              "           2,  4696,  9702,   707,     1,   302,  1656,    10,   155,\n",
              "          34,  2539,  1657,  4203,   707, 13961,     3,  4204,   465,\n",
              "           2,  2929,    10,  7561,     3, 13962,     3,  6266,   210,\n",
              "       13963,     6,   617,    34,  9703,  9704,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "execution_count": 345,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_data[0][40:200]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd45311a",
      "metadata": {
        "id": "cd45311a"
      },
      "source": [
        "### Pretrained embedding using glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d231965",
      "metadata": {
        "id": "4d231965",
        "outputId": "3216eea4-6e5e-4b64-8c91-1ee36317023c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sandberger 0.28365 -0.6263 -0.44351 0.2177 -0.087421 -0.17062 0.29266 -0.024899 0.26414 -0.17023 0.25817 0.097484 -0.33103 -0.43859 0.0095799 0.095624 -0.17777 0.38886 0.27151 0.14742 -0.43973 -0.26588 -0.024271 0.27186 -0.36761 -0.24827 -0.20815 0.22128 -0.044409 0.021373 0.24594 0.26143 0.29303 0.13281 0.082232 -0.12869 0.1622 -0.22567 -0.060348 0.28703 0.11381 0.34839 0.3419 0.36996 -0.13592 0.0062694 0.080317 0.0036251 0.43093 0.01882 0.31008 0.16722 0.074112 -0.37745 0.47363 0.41284 0.24471 0.075965 -0.51725 -0.49481 0.526 -0.074645 0.41434 -0.1956 -0.16544 -0.045649 -0.40153 -0.13136 -0.4672 0.18825 0.2612 0.16854 0.22615 0.62992 -0.1288 0.055841 0.01928 0.024572 0.46875 0.2582 -0.31672 0.048591 0.3277 -0.50141 0.30855 0.11997 -0.25768 -0.039867 -0.059672 0.5525 0.13885 -0.22862 0.071792 -0.43208 0.5398 -0.085806 0.032651 0.43678 -0.82607 -0.15701\n",
            "\n",
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = os.path.join(\n",
        " \"./glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file,encoding=\"UTF-8\") as f:\n",
        "    for line in f:\n",
        "        word, weights = line.split(maxsplit=1)\n",
        "        weights = np.fromstring(weights,sep=\" \",)\n",
        "        embeddings_index[word] = weights\n",
        "print(line)\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d341dc43",
      "metadata": {
        "id": "d341dc43"
      },
      "outputs": [],
      "source": [
        "num_tokens = vocab_size + 2 ## not sure why it is +2\n",
        "embedding_dim = 100  ## \n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3cecfe",
      "metadata": {
        "id": "1a3cecfe"
      },
      "outputs": [],
      "source": [
        "for word, i in word2i.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        \n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6cbc492",
      "metadata": {
        "id": "d6cbc492",
        "outputId": "838eac5d-9d2b-4400-cc01-ef2630c95518"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26187"
            ]
          },
          "execution_count": 354,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf0e642",
      "metadata": {
        "id": "0cf0e642",
        "outputId": "ecc80802-dec0-4110-f907-d7b09a3aac93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26184"
            ]
          },
          "execution_count": 355,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(word2i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b295d35",
      "metadata": {
        "id": "5b295d35"
      },
      "outputs": [],
      "source": [
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56d2e1f",
      "metadata": {
        "id": "a56d2e1f"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6989d271",
      "metadata": {
        "id": "6989d271"
      },
      "outputs": [],
      "source": [
        "num_samples = len(encoder_seq)\n",
        "num_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c43216",
      "metadata": {
        "id": "48c43216"
      },
      "outputs": [],
      "source": [
        "## encoder_data = encoder_data.reshape(-1,2000,498)\n",
        "## decoder_data = decoder_data.reshape(-1,2000,26)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cded7be5",
      "metadata": {
        "id": "cded7be5"
      },
      "outputs": [],
      "source": [
        "encoder_data =  tf.convert_to_tensor(encoder_data,dtype=float)\n",
        "decoder_data = tf.convert_to_tensor(decoder_data,dtype=float)\n",
        "encoder_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adf255a2",
      "metadata": {
        "scrolled": true,
        "id": "adf255a2"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "latent_dim = 128\n",
        "encoder_inputs = Input(shape=(2000,498))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_data)\n",
        "\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(2000,26))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_data,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(max_len_dec, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603c0954",
      "metadata": {
        "id": "603c0954"
      },
      "outputs": [],
      "source": [
        "latent_dim = 128\n",
        "\n",
        "encoder_inputs = Input(shape=(2000,498))\n",
        "print(encoder_data.shape)\n",
        "x = embedding_layer(encoder_inputs)\n",
        "print(x.shape)\n",
        "x, state_h, state_c = LSTM(latent_dim,\n",
        "                           return_state=True)(x)\n",
        "print(x.shape)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "\n",
        "\n",
        "decoder_inputs = Input(shape=(2000,26))\n",
        "x = embedding_layer(decoder_data)\n",
        "print(x.shape)\n",
        "x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\n",
        "print(x.shape)\n",
        "decoder_outputs = Dense(128, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ab8b12",
      "metadata": {
        "id": "02ab8b12"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
        "# rather than sequences of integers like `decoder_input_data`!\n",
        "model.fit([encoder_data, decoder_data], decoder_outputs,\n",
        "          epochs=5)\n",
        "What if I don't want to use teacher forcing for training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a587c9",
      "metadata": {
        "id": "d6a587c9"
      },
      "outputs": [],
      "source": [
        "tf.convert_to_tensor(encoder_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e91cc9de",
      "metadata": {
        "id": "e91cc9de"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}